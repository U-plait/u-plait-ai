from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.schemas import ChatTurnRequest
from app.core.jwt_auth import get_current_user_id
from app.core.database import get_db
from app.models.models import ChatLog
from app.service.rag_service import build_multi_turn_chain
from langchain_community.vectorstores.pgvector import PGVector
import os
import json
import asyncio
from langchain_openai import OpenAIEmbeddings
from fastapi.responses import StreamingResponse
from app.service.tag_service import update_user_tags


router = APIRouter()

@router.post("/chat")
async def chat_turn(
    request: ChatTurnRequest,
    db: Session = Depends(get_db),
    user_id: int = Depends(get_current_user_id)
):
    # ì´ì „ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°
    history = db.query(ChatLog).filter(ChatLog.user_id == user_id).order_by(ChatLog.created_at).all()
    history_pairs = []
    for i in range(0, len(history) -1, 2):
            question = history[i].log
            answer = history[i + 1].log
            history_pairs.append((question, answer))

    # *****ë¡œê·¸ ì¶œë ¥ìš©*****
    embedding = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = PGVector(
    connection_string=os.getenv("DATABASE_URL"),
    embedding_function=embedding,
    collection_name="plan_vector",
    )   
    # 1. ìœ ì‚¬ ë¬¸ì„œ ì§ì ‘ ê²€ìƒ‰ (ì¶œë ¥ìš©)
    docs = vectorstore.similarity_search(request.query, k=5)

    # 2. ë¡œê·¸ ì¶œë ¥
    print("\nğŸ“š [Retrieved Documents]")
    docs_scores = vectorstore.similarity_search_with_score(request.query, k=5)
    print(f"\n[DEBUG] Retrieved {len(docs_scores)} documents.")
    for i, (doc, score) in enumerate(docs_scores):
        print(f"Rank {i+1}: Score={score:.3f} | {doc.page_content[:100]}")
    # *****ë¡œê·¸ ì¶œë ¥ìš©*****

    # LangChain chain ìƒì„±
    chain = build_multi_turn_chain()

    # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
    async def gpt_stream():
         answer_buffer = ""  #GPT ë‹µë³€
         plan_json_buffer = ""   # plan_ids
         is_plan_mode = False  # [END_OF_MESSAGE] ì´í›„ plan_ids ì¡´ì¬ ì—¬ë¶€
    
         # LangChain chainì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ë©´ì„œ í† í° ë‹¨ìœ„ë¡œ ì‘ë‹µ ìˆ˜ì‹ 
         async for chunk in chain.astream({
              "question": request.query,
              "chat_history": history_pairs
         }):

            # chunkê°€ dictì´ë©´ 'answer' í•„ë“œì—ì„œ ê°€ì ¸ì˜¤ê³ , ì•„ë‹ˆë©´ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
            token = chunk.get("answer", "") if isinstance(chunk, dict) else str(chunk)

            if "[END_OF_MESSAGE]" in token:
                is_plan_mode = True
                parts = token.split("[END_OF_MESSAGE]")

                # ë©”ì‹œì§€ ë¶€ë¶„ë§Œ ìŠ¤íŠ¸ë¦¬ë°
                answer_buffer += parts[0]
                for char in parts[0]:
                    yield f"data: {char}\n\n"
                    await asyncio.sleep(0.01)

                yield f"data: [END_OF_MESSAGE]\n\n"

                # JSONì´ ê°™ì´ ë¶™ì–´ì˜¨ ê²½ìš° ì €ì¥
                if len(parts) > 1:
                    plan_json_buffer += parts[1]
                    answer_buffer += "[END_OF_MESSAGE]" + parts[1]  # ğŸ”¥ ì—¬ê¸°ë¥¼ ì¶”ê°€
                continue

            # ì´ ì‹œì ì˜ tokenì€ [END_OF_MESSAGE] ì—†ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸
            if not is_plan_mode:
                answer_buffer += token
                for char in token:
                    yield f"data: {char}\n\n"
                    await asyncio.sleep(0.01)
            else:
                plan_json_buffer += token

        # plan_ids íŒŒì‹±
         try:
            plan_data = json.loads(plan_json_buffer.strip())
         except Exception:
            plan_data = {"plan_ids": []}

        # plan_ids JSONì„ ìŠ¤íŠ¸ë¦¬ë° ì „ì†¡ (ì¶”í›„ì— ì´ê±° ê¸°ë°˜ìœ¼ë¡œ dbì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ê±¸ë¡œ ê³ ì³ì•¼í•¨)
         yield f"data: {json.dumps(plan_data)}\n\n"

        # ğŸ”¥ ìœ ì € íƒœê·¸ ì—…ë°ì´íŠ¸ í˜¸ì¶œ ì¶”ê°€
         if plan_data.get("plan_ids"):
            update_user_tags(user_id=user_id, plan_ids=plan_data["plan_ids"], db=db)

        # ëŒ€í™” ë¡œê·¸ DBì— ì €ì¥ (ì§ˆë¬¸ + ë‹µë³€)
         seq = len(history)+1
         db.add(ChatLog(user_id=user_id, log=request.query, is_chatbot=False))
         db.add(ChatLog(user_id=user_id, log=answer_buffer, is_chatbot=True))
         db.commit()
    return StreamingResponse(gpt_stream(), media_type="text/event-stream")